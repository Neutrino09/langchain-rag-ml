

A clean, modular Retrieval-Augmented Generation (RAG) application built using **LangChain**, **OpenAI embeddings**, **FAISS**, and a **Streamlit chat UI**.
It answers questions about Machine Learning using a custom knowledge base extracted from the Machine Learning Wikipedia page.

This project was built as part of the **Generative AI Engineer Intern Assignment** for **Pythrust Technologies**.

---

## **ğŸ“Œ Features**

* **Custom Knowledge Base**
  Ingests the Machine Learning Wikipedia page (stored locally as `.txt`).

* **Document Processing Pipeline**

  * Text loading
  * Chunking (Recursive Character Splitter)
  * Embedding generation using **OpenAI text-embedding-3-small**
  * Vector store creation with **FAISS**

* **Retrieval-Augmented Generation (RAG)**

  * FAISS retriever
  * RetrievalQA chain (LangChain)
  * `ChatOpenAI` model for grounded response generation
  * Source citation for each answer

* **Streamlit UI**

  * Clean, modern, production-style chat interface
  * Conversation memory
  * Source tags for transparency
  * Side panel with app info and usage instructions

* **Modular Codebase**
  Each responsibility (config, indexing, retrieval, UI) is separated into dedicated modules.
  <img width="1470" height="956" alt="Screenshot 2025-11-16 at 6 10 34â€¯PM" src="https://github.com/user-attachments/assets/882618e3-7d86-4add-a8e9-5a1941e21a78" />


---

## **ğŸ“ Project Structure**

```
langchain-rag-ml/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ machine_learning.txt        â† Knowledge base
â”‚
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ faiss_index/                    â† Auto-generated FAISS index
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app_streamlit.py                â† Streamlit chat UI
â”‚   â”œâ”€â”€ config.py                       â† Env-based config (OpenAI key, paths, settings)
â”‚   â”œâ”€â”€ data_loader.py                  â† Loads text files from data/raw/
â”‚   â”œâ”€â”€ index_builder.py                â† Chunking, embedding, FAISS index creation
â”‚   â”œâ”€â”€ rag_chain.py                    â† RAG pipeline + ask_question() API
â”‚   â”œâ”€â”€ build_index.py                  â† Script to build the FAISS index
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## **âš™ï¸ Installation**

Clone the repository:

```bash
git clone https://github.com/Neutrino09/langchain-rag-ml.git
cd langchain-rag-ml
```

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## **ğŸ”‘ Set OpenAI API Key**

The app reads your API key from an **environment variable**, so set it like this:

### macOS / Linux:

```bash
export OPENAI_API_KEY="your-key-here"
```

### Windows (Powershell):

```powershell
setx OPENAI_API_KEY "your-key-here"
```

> Do **not** hardcode your key into the config file if pushing to GitHub.

---

## **ğŸ“š Step 1 â€” Prepare the Dataset**

Place your source documents in:

```
data/raw/
```

By default, the repo comes with:

```
machine_learning.txt
```

You can add more `.txt` files â€” the pipeline will ingest all of them automatically.

---

## **ğŸ§± Step 2 â€” Build the FAISS Index**

Before running the app, embed your documents and create the vector index:

```bash
python src/build_index.py
```

This generates:

```
artifacts/faiss_index/
```

---

## **ğŸ’¬ Step 3 â€” Run the Streamlit App**

```bash
streamlit run src/app_streamlit.py
```

Then open:

```
http://localhost:8501
```

You can now chat with the RAG system.

---

## **ğŸ§  How It Works**

1. **Ingestion**
   `data_loader.py` loads all `.txt` files under `data/raw/`.

2. **Chunking & Embeddings**
   `index_builder.py` splits documents, generates embeddings (OpenAI), and builds FAISS index.

3. **Retrieval**
   `rag_chain.py` loads the FAISS index and retrieves the top-k relevant chunks for each query.

4. **Generation**
   A `RetrievalQA` chain calls the LLM (`gpt-4o-mini`) with the retrieved context.

5. **UI**
   `app_streamlit.py` provides a chat interface and displays source citations.

---

## **ğŸ“ Notes**

* The entire pipeline is modular and easy to extend:

  * Swap in a different LLM
  * Add more documents
  * Change vector store or retriever
  * Integrate a proper memory module

* The FAISS index is not stored in git; itâ€™s generated locally.

---

